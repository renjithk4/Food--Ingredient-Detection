# -*- coding: utf-8 -*-
"""OPT and Image  from  text generation_food.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13ViaFlcfDuDEIvesjv_WaH47npDxPNzD
"""

from google.colab import drive
drive.mount('/content/drive')



import pandas as pd

file_path = '/content/drive/MyDrive/1_food/ingredients_new.csv'

import pandas as pd

# Read CSV into DataFrame
df = pd.read_csv(file_path)

# Display the first few rows of the DataFrame
print(df.head())

df

# Access and display the first row of the ingredients column
first_ingredient = df.loc[0, 'ingredients']

print("First row of ingredients column:")
print(type(first_ingredient))

# Convert string representation of list back to list
import ast
df['ingredients'] = df['ingredients'].apply(ast.literal_eval)

# Access and display the first row of the ingredients column
first_ingredient = df.loc[0, 'ingredients']

print("First row of ingredients column:")
print(type(first_ingredient))
print(first_ingredient)

import ast
df['instructions'] = df['instructions'].apply(ast.literal_eval)

# Access and display the first row of the instruction column
first_instruction = df.loc[0, 'instructions']

print("First row of instructions column:")
print(type(first_ingredient))
print(first_instruction)

import pandas as pd
from transformers import GPT2Tokenizer, GPT2LMHeadModel

# Assuming df is your DataFrame
df_test = df.head(1)  # Select only the first row for testing

# Initialize GPT-2 tokenizer and model
#tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
#model = GPT2LMHeadModel.from_pretrained('gpt2')

# Generate prompts for each combination of title, ingredients, and instructions
generated_prompts_test = []

# Iterate through the rows
for i in range(len(df_test)):
    row = df_test.iloc[i]  # Accessing rows by index
    title = row['title']
    ingredients = row['ingredients']  # Assuming this is a list of ingredients
    instructions = row['instructions']  # Assuming this is a list of instructions

    # Format ingredients and instructions as strings
    ingredients_str = '\n'.join(ingredients)
    instructions_str = '\n'.join(instructions)

    # Create a prompt with the title, ingredients, and instructions
    prompt = f"Construct a single image with the following details:\n\nTitle: {title}\n\nIngredients:\n{ingredients_str}\n\nInstructions:\n{instructions_str}"
    generated_prompts_test.append(prompt)

# Print the prompts in list format
print("prompts = [")
for prompt in generated_prompts_test:
    print(f'"{prompt}",')
print("]")

# Example: Print the first few generated prompts
for i, prompt in enumerate(generated_prompts_test):
    print(f"Prompt {i+1}: {prompt}")

import pandas as pd
from transformers import GPT2Tokenizer, GPT2LMHeadModel



# Select only the first row for testing
df_test = df.head(1)

# Initialize GPT-2 tokenizer and model
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
model = GPT2LMHeadModel.from_pretrained('gpt2')

# Generate prompts for each combination of title and ingredients
generated_prompts_test = []
num_iterations = len(df_test)

# Iterate through the rows
for i in range(num_iterations):
    # Get title and ingredients from the current row
    row = df_test.iloc[i]  # Accessing rows by index
    title = row['title']
    ingredients = row['ingredients']


    # Create prompts for title + each ingredient
    for ingredient in ingredients:
        prompt = f"{title} AND {ingredient}. "
        generated_prompts_test.append(prompt)

# Print the prompts in list format
print("prompts = [")
for prompt in generated_prompts_test:
    print(f'"{prompt}",')
print("]")

# Example: Print the first few generated prompts
for i, prompt in enumerate(generated_prompts_test):
    print(f"Prompt {i+1}: {prompt}")

import pandas as pd
from transformers import GPT2Tokenizer, GPT2LMHeadModel



# Select only the first row for testing
df_test = df.head(5)

# Initialize GPT-2 tokenizer and model
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
model = GPT2LMHeadModel.from_pretrained('gpt2')

# Generate prompts for each combination of title and ingredients
generated_prompts_test = []
num_iterations = len(df_test)

# Iterate through the rows
for i in range(num_iterations):
    # Get title and ingredients from the current row
    row = df_test.iloc[i]  # Accessing rows by index
    title = row['title']
    ingredients = row['ingredients']


    # Create prompts for title + each ingredient
    for ingredient in ingredients:
        prompt = f"{title} AND {ingredient}. "
        generated_prompts_test.append(prompt)

# Print the prompts in list format
print("prompts = [")
for prompt in generated_prompts_test:
    print(f'"{prompt}",')
print("]")

# Example: Print the first few generated prompts
for i, prompt in enumerate(generated_prompts_test):
    print(f"Prompt {i+1}: {prompt}")

from transformers import GPT2Tokenizer, GPT2LMHeadModel
import torch

# Load GPT-2 tokenizer and model
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
model = GPT2LMHeadModel.from_pretrained('gpt2')

# Function to generate prompts
def generate_prompt(input_text, max_length=50):
    # Encode the input text
    inputs = tokenizer.encode(input_text, return_tensors='pt')

    # Generate text using the model
    outputs = model.generate(inputs, max_length=max_length, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)

    # Decode the generated text
    prompt = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return prompt

# Generate prompts for the first few inputs
generated_prompts = []
for i, input_text in enumerate(generated_prompts_test):  # Limiting to the first 5 inputs for demonstration
    prompt = generate_prompt(input_text)
    generated_prompts.append(prompt)
    print(f"Generated Prompt {i+1}: {prompt}")

#Installing required packages

!pip install git+https://github.com/huggingface/transformers

!pip install git+https://github.com/huggingface/peft.git

!pip install torch
!pip install -q bitsandbytes accelerate

########## IN CASE OF BELOW ERROR WHEN INSTALLING PACKAGES USING PIP, RUN THIS CELL #####################

#---NotImplementedError: A UTF-8 locale is required. Got ANSI_X3.4-1968---#

import locale
locale.getpreferredencoding = lambda: "UTF-8"

#Importing libraries

from peft import PeftConfig, PeftModel
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
import torch
import re

from huggingface_hub import login
login(token="")

#Loading adapter model and merging it with base model for inferencing

torch.set_default_device('cuda')

peft_model_id = "abhishek7/Prompt_diffusion-v0.1"

config = PeftConfig.from_pretrained(peft_model_id)

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,
)

model = AutoModelForCausalLM.from_pretrained(
    config.base_model_name_or_path,
    low_cpu_mem_usage=True,
    quantization_config=bnb_config,
    torch_dtype=torch.float16,
    device_map="auto"
)

model = PeftModel.from_pretrained(model, peft_model_id)

model = model.merge_and_unload()

tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path, trust_remote_code=True)
tokenizer.padding_side = "right"



# Function to truncate text based on punctuation count

def truncate_text(text, max_punctuation):
    punctuation_count = 0
    truncated_text = ""
    for char in text:
        truncated_text += char
        if char in [',', '.']:
            punctuation_count += 1
            if punctuation_count >= max_punctuation:
                break
    # Replace the last comma with a full stop if the last punctuation is a comma
    if truncated_text.rstrip()[-1] == ',':
        truncated_text = truncated_text.rstrip()[:-1] + '.'

    return truncated_text


# Function to generate prompt

def generate_prompt(input, max_length, temperature):
    input_context = f'''
###Human:
generate a stable diffusion prompt for {input}

###Assistant:
'''

    inputs = tokenizer.encode(input_context, return_tensors="pt")
    outputs = model.generate(inputs, max_length=max_length, temperature=temperature, num_return_sequences=1, do_sample=True)

    output_text = tokenizer.decode(outputs[0], skip_special_tokens = True)

    # Extract the Assistant's response using regex
    match = re.search(r'###Assistant:(.*?)(###Human:|$)', output_text, re.DOTALL)
    if match:
        assistant_response = match.group(1)
    else:
        raise ValueError("No Assistant response found")



    # Truncate the Assistant's response based on the criteria, the output is truncated based on number of comma/full-stops.
    truncated_response = truncate_text(assistant_response, max_punctuation=10)

    return truncated_response

# Configure logging to suppress warnings
import logging
logging.getLogger("transformers").setLevel(logging.ERROR)

# Usage:



prompts = [
"Worlds Best Mac and Cheese AND 6 ounces penne.",
"Worlds Best Mac and Cheese AND 2 cups Beechers Flagship Cheese Sauce.",
"Worlds Best Mac and Cheese AND 1 ounce Cheddar, grated.",
"Worlds Best Mac and Cheese AND 1 ounce Gruyere cheese, grated.",
"Worlds Best Mac and Cheese AND 1/4 to 1/2 teaspoon chipotle chili powder.",
"Worlds Best Mac and Cheese AND 1/4 cup unsalted butter.",
"Worlds Best Mac and Cheese AND 1/3 cup all-purpose flour.",
"Worlds Best Mac and Cheese AND 3 cups milk.",
"Worlds Best Mac and Cheese AND 14 ounces semihard cheese, grated.",
"Worlds Best Mac and Cheese AND 2 ounces semisoft cheese, grated.",
"Worlds Best Mac and Cheese AND 1/2 teaspoon kosher salt.",
"Worlds Best Mac and Cheese AND 1/4 to 1/2 teaspoon chipotle chili powder.",
"Worlds Best Mac and Cheese AND 1/8 teaspoon garlic powder.",
"Worlds Best Mac and Cheese AND .",
"Dilly Macaroni Salad Recipe AND 1 c. elbow macaroni.",
"Dilly Macaroni Salad Recipe AND 1 c. cubed American cheese.",
"Dilly Macaroni Salad Recipe AND 1/2 c. sliced celery.",
"Dilly Macaroni Salad Recipe AND 1/2 c. minced green pepper.",
"Dilly Macaroni Salad Recipe AND 3 tbsp. minced pimento.",
"Dilly Macaroni Salad Recipe AND 1/2 c. mayonnaise or possibly salad dressing.",
"Dilly Macaroni Salad Recipe AND 1 tbsp. vinegar.",
"Dilly Macaroni Salad Recipe AND 3/4 teaspoon salt.",
"Dilly Macaroni Salad Recipe AND 1/2 teaspoon dry dill weed.",
"Gazpacho AND 8 tomatoes, quartered.",
"Gazpacho AND Kosher salt.",
"Gazpacho AND 1 red onion, cut into small dice.",
"Gazpacho AND 1 green bell pepper, cut into small dice.",
"Gazpacho AND 1 red bell pepper, cut into small dice.",
"Gazpacho AND 1 yellow bell pepper, cut into small dice.",
"Gazpacho AND 1/2 cucumber, cut into small dice.",
"Gazpacho AND Extra-virgin olive oil, for drizzling.",
"Gazpacho AND 3 leaves fresh basil, finely chopped.",
"Crunchy Onion Potato Bake AND 2 12 cups milk.",
"Crunchy Onion Potato Bake AND 1 12 cups water.",
"Crunchy Onion Potato Bake AND 14 cup butter.",
"Crunchy Onion Potato Bake AND mashed potatoes, 1 box, homestyle.",
"Crunchy Onion Potato Bake AND 1 can whole kernel corn.",
"Crunchy Onion Potato Bake AND 1 cup cheddar cheese.",
"Crunchy Onion Potato Bake AND 1 cup French-fried onions.",
"Cool 'n Easy Creamy Watermelon Pie AND 1 package watermelon gelatin.",
"Cool 'n Easy Creamy Watermelon Pie AND 14 cup boiling water.",
"Cool 'n Easy Creamy Watermelon Pie AND 1 package Cool Whip, thawed.",
"Cool 'n Easy Creamy Watermelon Pie AND 2 cups cubed seedless watermelon.",
"Cool 'n Easy Creamy Watermelon Pie AND 1 graham cracker crust.",
]

# Loop through the prompts list and apply generate_prompt to each element
generated_texts = []
for prompt in prompts:
    generated_text = generate_prompt(prompt, 150, 1)
    generated_texts.append(f'"{generated_text.strip()}"')

# Print the results as a list of strings
print("prompts = [")
for text in generated_texts:
    print(text + ",")
print("]")

!pip install daam==0.0.12

"""                                    
 ## START FROM HERE







"""







!pip install git+https://github.com/RishiDarkDevil/TITAN.git

!pip install "jax[cuda12_pip]==0.4.23" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html

# For Stable Diffusion
from diffusers import StableDiffusionPipeline

# For Heatmap Generation
import daam

# For TITAN workflow
from titan import *

rm -r /content/Data-Generated



prompts=[]

# List of prompts
prompts = ['tamari',
 'mincemeat',
 'flour tortilla',
 'breadcrumbs',
 'oyster sauce',
 'red snapper',
 'delicata squash',
 'tea',
 'poultry seasoning',
 'cracker meal']


# Load PromptHandler from TITAN
prompt_handler = PromptHandler()

# Filter out the objects from the prompts to be used for annotations
processed_prompts = prompt_handler.clean_prompt(prompts)

print(processed_prompts)

# Diffusion Model Setup
DIFFUSION_MODEL_PATH = 'stabilityai/stable-diffusion-2-base'
DEVICE = 'cuda' # device
NUM_IMAGES_PER_PROMPT = 2 # Number of images to be generated per prompt
NUM_INFERENCE_STEPS = 50 # Number of inference steps to the Diffusion Model
SAVE_AFTER_NUM_IMAGES = 1 # Number of images after which the annotation and caption files will be saved

# Load Model
model = StableDiffusionPipeline.from_pretrained(DIFFUSION_MODEL_PATH)
model = model.to(DEVICE) # Set it to something else if needed, make sure DAAM supports that

NUM_IMAGES_PER_PROMPT = 2

# The TITAN Dataset
titan_dataset = TITANDataset()


# Generating and Annotating Generated Images
try:

  # Iterating over the processed_prompts
  for i, processed_prompt in enumerate(processed_prompts):

    # Generating images for these processed prompts and annotating them
    for j in range(NUM_IMAGES_PER_PROMPT):

      # traversing the processed prompts
      prompt, _, _ = processed_prompt

      print()
      print(f'Prompt No.: {i+1}/{len(processed_prompts)}')
      print(f'Image No.: {j+1}/{NUM_IMAGES_PER_PROMPT}')
      print('Generating Image...')

      # generating images. keeping track of the attention heatmaps
      with daam.trace(model) as trc:
        output_image = model(prompt, num_inference_steps=NUM_INFERENCE_STEPS).images[0]
        global_heat_map = trc.compute_global_heat_map()

      # Saving Generated Image
      output_image.save(os.path.join(titan_dataset.image_dir, f'{i}_{j}.png'))
      print(f'Saved Generated Image... {i}_{j}.png')

      # Object Annotate Generated Image using the attention heatmaps
      print(f'Adding Annotation for {i}_{j}.png')
      titan_dataset.annotate(output_image, f'{i}_{j}.png', global_heat_map, processed_prompt)

      if len(titan_dataset.images) % SAVE_AFTER_NUM_IMAGES == 0:
        print()
        # Saving Annotations on Disk
        titan_dataset.save()
        # Freeing up Memory
        titan_dataset.clear()

  if len(titan_dataset.annotations):
    titan_dataset.save()
    titan_dataset.clear()

except KeyboardInterrupt: # In case of KeyboardInterrupt save the annotations and captions
  titan_dataset.save()
  titan_dataset.clear()

# merge annotation and caption files
merge_annotation_files()
merge_caption_files()

#!unzip "/content/Data-Generated.zip"

# Load the Visualizer
titan_visualizer = TITANViz()

# Interactive Annotation Visualizer
titan_visualizer.visualize_annotation(image_id = 13)

import os
import json
from sklearn.model_selection import train_test_split

# Paths to your folders
image_folder = 'Data-Generated/images'
annotation_folder = 'Data-Generated/annotations'
caption_folder = 'Data-Generated/captions'

# Get lists of files
image_files = sorted([f for f in os.listdir(image_folder) if f.endswith('.png')])
annotation_files = sorted([f for f in os.listdir(annotation_folder) if f.endswith('.json')])
caption_files = sorted([f for f in os.listdir(caption_folder) if f.endswith('.json')])

# Create a list of tuples (image_file, annotation_file, caption_file)
file_tuples = []
for img_file in image_files:
    idx = img_file.split('.')[0]
    annot_file = f'object-detect-{int(idx.split("_")[0])*2 + int(idx.split("_")[1]) + 1}.json'
    cap_file = f'object-caption-{int(idx.split("_")[0])*2 + int(idx.split("_")[1]) + 1}.json'
    file_tuples.append((img_file, annot_file, cap_file))

# Split data into train and test sets
train_files, test_files = train_test_split(file_tuples, test_size=0.2, random_state=42)

# Print results
print(f"Number of training samples: {len(train_files)}")
print(f"Number of testing samples: {len(test_files)}")

# Save the file lists to text files
with open('train_files.txt', 'w') as f:
    for img, annot, cap in train_files:
        f.write(f"{img}\t{annot}\t{cap}\n")

with open('test_files.txt', 'w') as f:
    for img, annot, cap in test_files:
        f.write(f"{img}\t{annot}\t{cap}\n")

import os

# Define paths
base_dir = 'Data-Generated'
train_dir = os.path.join(base_dir, 'train')
test_dir = os.path.join(base_dir, 'test')

# Subdirectories
subdirs = ['Images', 'Annotations', 'Captions']

# Create directories
for dir_path in [train_dir, test_dir]:
    os.makedirs(dir_path, exist_ok=True)
    for subdir in subdirs:
        os.makedirs(os.path.join(dir_path, subdir), exist_ok=True)

"""End here"""









#rm -r "/content/Data-Generated-Train"

















file_path = '/content/drive/MyDrive/1_food/All Text data/train_data.json'

import os
import json



# Check the size of the file
file_size = os.path.getsize(file_path)
print(f"Size of the file: {file_size} bytes")

# Load the JSON file
with open(file_path, 'r') as file:
    data = json.load(file)

# Print the first 3-4 entries
print("First 3-4 entries of the JSON file:")
for i, entry in enumerate(data[:4]):
    print(f"Entry {i+1}: {entry}")

# Convert to a format for further evaluation (e.g., a list of dictionaries)
data_list = [entry for entry in data]

# Print the type of data_list to confirm conversion
print(f"Type of data_list: {type(data_list)}")

# Determine if the data is a list or a dictionary
if isinstance(data, list):
    # Print the first 3-4 entries
    print("First 3-4 entries of the JSON file:")
    for i, entry in enumerate(data[:4]):
        print(f"Entry {i+1}: {entry}")

    # Convert to a format for further evaluation (e.g., a list of dictionaries)
    data_list = data

elif isinstance(data, dict):
    # Print the first 3-4 entries
    print("First 3-4 entries of the JSON file:")
    for i, (key, value) in enumerate(data.items()):
        if i >= 4:
            break
        print(f"Entry {i+1}: {key}: {value}")

    # Convert to a format for further evaluation (e.g., a list of dictionaries)
    data_list = [{key: value} for key, value in data.items()]

import pandas as pd
import json

# Function to read the JSON file and convert it to a list of dictionaries
def read_json_file(filepath):
    with open(filepath, 'r') as file:
        data = json.load(file)
    return data

# Function to convert the list of JSON entries to a pandas DataFrame
def json_to_dataframe(json_data):
    records = []
    for entry_id, entry_data in json_data.items():
        record = {'id': entry_id}
        record.update(entry_data)
        records.append(record)
    df = pd.DataFrame(records)
    return df

# Assuming your JSON file is named 'test_data.json' and located in the same folder
json_filepath = '/content/drive/MyDrive/1_food/All Text data/test_data.json'

# Load and convert JSON data to DataFrame
json_data = read_json_file(json_filepath)
df = json_to_dataframe(json_data)

# Print the DataFrame to see the result
print(df)

df

# Flatten the list of ingredients and remove None values
all_ingredients = [ingredient for sublist in df['ing_text_18k'] for ingredient in sublist if ingredient is not None]

# Get unique ingredients
unique_ingredients = list(set(all_ingredients))

# Print the unique ingredients
print(unique_ingredients)

len(unique_ingredients)

import pandas as pd
import json

# Function to read the JSON file and convert it to a list of dictionaries
def read_json_file(filepath):
    with open(filepath, 'r') as file:
        data = json.load(file)
    return data

# Function to convert the list of JSON entries to a pandas DataFrame
def json_to_dataframe(json_data):
    records = []
    for entry_id, entry_data in json_data.items():
        record = {'id': entry_id}
        record.update(entry_data)
        records.append(record)
    df = pd.DataFrame(records)
    return df

# Assuming your JSON file is named 'test_data.json' and located in the same folder
json_filepath = '/content/drive/MyDrive/1_food/All Text data/train_data.json'

# Load and convert JSON data to DataFrame
json_data = read_json_file(json_filepath)
df = json_to_dataframe(json_data)

# Print the DataFrame to see the result
print(df)

df

# Flatten the list of ingredients and remove None values
all_ingredients1 = [ingredient for sublist in df['ing_text_18k'] for ingredient in sublist if ingredient is not None]

# Get unique ingredients
unique_ingredients1 = list(set(all_ingredients1))

# Print the unique ingredients
print(unique_ingredients1)

len(unique_ingredients1)

import pandas as pd
import json

# Function to read the JSON file and convert it to a list of dictionaries
def read_json_file(filepath):
    with open(filepath, 'r') as file:
        data = json.load(file)
    return data

# Function to convert the list of JSON entries to a pandas DataFrame
def json_to_dataframe(json_data):
    records = []
    for entry_id, entry_data in json_data.items():
        record = {'id': entry_id}
        record.update(entry_data)
        records.append(record)
    df = pd.DataFrame(records)
    return df

# Assuming your JSON file is named 'test_data.json' and located in the same folder
json_filepath = '/content/drive/MyDrive/1_food/All Text data/val_data.json'

# Load and convert JSON data to DataFrame
json_data = read_json_file(json_filepath)
df = json_to_dataframe(json_data)

# Print the DataFrame to see the result
print(df)

# Flatten the list of ingredients and remove None values
all_ingredients2 = [ingredient for sublist in df['ing_text_18k'] for ingredient in sublist if ingredient is not None]

# Get unique ingredients
unique_ingredients2 = list(set(all_ingredients2))

# Print the unique ingredients
print(unique_ingredients2)

print(len(unique_ingredients2))

# Check for a specific element
element_to_check = 'recipe'
if element_to_check in combined_unique_list:
    print(f"'{element_to_check}' is in the list of unique ingredients.")
else:
    print(f"'{element_to_check}' is NOT in the list of unique ingredients.")

# Combine the lists and remove duplicates by converting to a set and then back to a list
combined_unique_list = list(set(unique_ingredients + unique_ingredients1+unique_ingredients2))

# Print the combined unique list
print("Combined Unique List:", combined_unique_list)

len(combined_unique_list)

!pip show transformers

import pandas as pd

# Load the CSV file
csv_file = 'ingredients_list.csv'
df = pd.read_csv(csv_file)

# Assuming the prompts are in the first column
prompts = df.iloc[:, 0].tolist()

# Take the first 2 elements of that list just to test and see
prompts = prompts[:2]

prompts

import os
import shutil

# For Stable Diffusion
from diffusers import StableDiffusionPipeline

# For Heatmap Generation
import daam

# For TITAN workflow
from titan import *
import pandas as pd

# Load the CSV file
csv_file = 'ingredients_list.csv'
df = pd.read_csv(csv_file)

# Assuming the prompts are in the first column
prompts = df.iloc[:, 0].tolist()

# Take the first 2 elements of that list just to test and see
prompts = prompts[:2]
# Load PromptHandler from TITAN
prompt_handler = PromptHandler()

# Filter out the objects from the prompts to be used for annotations
processed_prompts = prompt_handler.clean_prompt(prompts)

print(processed_prompts)

# Diffusion Model Setup
DIFFUSION_MODEL_PATH = 'stabilityai/stable-diffusion-2-base'
DEVICE = 'cuda' # device
NUM_IMAGES_PER_PROMPT = 3 # Number of images to be generated per prompt
NUM_INFERENCE_STEPS = 50 # Number of inference steps to the Diffusion Model
SAVE_AFTER_NUM_IMAGES = 1 # Number of images after which the annotation and caption files will be saved

# Load Model
model = StableDiffusionPipeline.from_pretrained(DIFFUSION_MODEL_PATH)
model = model.to(DEVICE) # Set it to something else if needed, make sure DAAM supports that

# The TITAN Dataset
titan_dataset = TITANDataset()

# Generating and Annotating Generated Images
try:
    # Iterating over the processed_prompts
    for i, processed_prompt in enumerate(processed_prompts):
        # Generating images for these processed prompts and annotating them
        for j in range(NUM_IMAGES_PER_PROMPT):
            # traversing the processed prompts
            prompt, _, _ = processed_prompt

            print()
            print(f'Prompt No.: {i+1}/{len(processed_prompts)}')
            print(f'Image No.: {j+1}/{NUM_IMAGES_PER_PROMPT}')
            print('Generating Image...')

            # generating images. keeping track of the attention heatmaps
            with daam.trace(model) as trc:
                output_image = model(prompt, num_inference_steps=NUM_INFERENCE_STEPS).images[0]
                global_heat_map = trc.compute_global_heat_map()

            # Saving Generated Image
            output_image.save(os.path.join(titan_dataset.image_dir, f'{i}_{j}.png'))
            print(f'Saved Generated Image... {i}_{j}.png')

            # Object Annotate Generated Image using the attention heatmaps
            print(f'Adding Annotation for {i}_{j}.png')
            titan_dataset.annotate(output_image, f'{i}_{j}.png', global_heat_map, processed_prompt)

            if len(titan_dataset.images) % SAVE_AFTER_NUM_IMAGES == 0:
                print()
                # Saving Annotations on Disk
                titan_dataset.save()
                # Freeing up Memory
                titan_dataset.clear()

    if len(titan_dataset.annotations):
        titan_dataset.save()
        titan_dataset.clear()

except KeyboardInterrupt: # In case of KeyboardInterrupt save the annotations and captions
    titan_dataset.save()
    titan_dataset.clear()

# Merge annotation and caption files
merge_annotation_files()
merge_caption_files()

# Define the new base directory for the restructured folders
NEW_BASE_DIR = 'New_Generated_Train'
NEW_IMAGES_DIR = os.path.join(NEW_BASE_DIR, 'Images')
NEW_ANNOTATIONS_DIR = os.path.join(NEW_BASE_DIR, 'Annotations')
NEW_CAPTIONS_DIR = os.path.join(NEW_BASE_DIR, 'Captions')

# Create the new folder structure
os.makedirs(NEW_IMAGES_DIR, exist_ok=True)
os.makedirs(NEW_ANNOTATIONS_DIR, exist_ok=True)
os.makedirs(NEW_CAPTIONS_DIR, exist_ok=True)

# Function to copy files to the new folder structure with renamed folders
def copy_files_to_new_structure(original_dir, new_dir, prefix, file_extension, num_per_prompt, prompts_list):
    file_counter = 1
    for prompt in prompts_list:
        prompt_dir = os.path.join(new_dir, f'{prompt}')
        os.makedirs(prompt_dir, exist_ok=True)
        for j in range(num_per_prompt):
            src_file = os.path.join(original_dir, f'{prefix}-{file_counter}{file_extension}')
            dst_file = os.path.join(prompt_dir, f'{prefix}{j+1}{file_extension}')
            if os.path.exists(src_file):
                shutil.copy(src_file, dst_file)
            file_counter += 1

# Copy images
for i, prompt in enumerate(prompts):
    prompt_dir = os.path.join(NEW_IMAGES_DIR, f'{prompt}')
    os.makedirs(prompt_dir, exist_ok=True)
    for j in range(NUM_IMAGES_PER_PROMPT):
        src_file = os.path.join(titan_dataset.image_dir, f'{i}_{j}.png')
        dst_file = os.path.join(prompt_dir, f'image{j+1}.png')
        if os.path.exists(src_file):
            shutil.copy(src_file, dst_file)

# Copy annotations
copy_files_to_new_structure(titan_dataset.annotation_dir, NEW_ANNOTATIONS_DIR, 'object-detect', '.json', NUM_IMAGES_PER_PROMPT, prompts)

# Copy captions
copy_files_to_new_structure(titan_dataset.caption_dir, NEW_CAPTIONS_DIR, 'object-caption', '.json', NUM_IMAGES_PER_PROMPT, prompts)

# Load the Visualizer
titan_visualizer = TITANViz()

# Interactive Annotation Visualizer
titan_visualizer.visualize_annotation(image_id=1)

import os
import shutil



# For Stable Diffusion
from diffusers import StableDiffusionPipeline

# For Heatmap Generation
import daam

# For TITAN workflow
from titan import *

from PIL import Image

# Read the CSV file to get the prompts
import pandas as pd

# Load prompts from the CSV file
csv_file_path = 'ingredients_list.csv'
df = pd.read_csv(csv_file_path)

# Assuming the prompts are in the first column
prompts = df.iloc[:, 0].tolist()

# Take the first 2 elements of that list just to test and see
prompts = prompts[:10]
# Load PromptHandler from TITAN
prompt_handler = PromptHandler()

# Filter out the objects from the prompts to be used for annotations
processed_prompts = prompt_handler.clean_prompt(prompts)

print(processed_prompts)

# Diffusion Model Setup
DIFFUSION_MODEL_PATH = 'stabilityai/stable-diffusion-2-base'
DEVICE = 'cuda'  # device
NUM_IMAGES_PER_PROMPT = 3  # Number of images to be generated per prompt
NUM_INFERENCE_STEPS = 50  # Number of inference steps to the Diffusion Model
SAVE_AFTER_NUM_IMAGES = 1  # Number of images after which the annotation and caption files will be saved
TARGET_SIZE = (224, 224)  # Desired size for the generated images

# Load Model
model = StableDiffusionPipeline.from_pretrained(DIFFUSION_MODEL_PATH)
model = model.to(DEVICE)  # Set it to something else if needed, make sure DAAM supports that

# The TITAN Dataset
titan_dataset = TITANDataset()

# Generating and Annotating Generated Images
try:
    # Iterating over the processed_prompts
    for i, processed_prompt in enumerate(processed_prompts):
        # Generating images for these processed prompts and annotating them
        for j in range(NUM_IMAGES_PER_PROMPT):
            # traversing the processed prompts
            prompt, _, _ = processed_prompt

            print()
            print(f'Prompt No.: {i + 1}/{len(processed_prompts)}')
            print(f'Image No.: {j + 1}/{NUM_IMAGES_PER_PROMPT}')
            print('Generating Image...')

            # generating images. keeping track of the attention heatmaps
            with daam.trace(model) as trc:
                output_image = model(prompt, num_inference_steps=NUM_INFERENCE_STEPS).images[0]
                global_heat_map = trc.compute_global_heat_map()

            # Resize the generated image
            output_image = output_image.resize(TARGET_SIZE, Image.ANTIALIAS)

            # Saving Generated Image
            output_image.save(os.path.join(titan_dataset.image_dir, f'{i}_{j}.png'))
            print(f'Saved Generated Image... {i}_{j}.png')

            # Object Annotate Generated Image using the attention heatmaps
            print(f'Adding Annotation for {i}_{j}.png')
            titan_dataset.annotate(output_image, f'{i}_{j}.png', global_heat_map, processed_prompt)

            if len(titan_dataset.images) % SAVE_AFTER_NUM_IMAGES == 0:
                print()
                # Saving Annotations on Disk
                titan_dataset.save()
                # Freeing up Memory
                titan_dataset.clear()

    if len(titan_dataset.annotations):
        titan_dataset.save()
        titan_dataset.clear()

except KeyboardInterrupt:  # In case of KeyboardInterrupt save the annotations and captions
    titan_dataset.save()
    titan_dataset.clear()

# Merge annotation and caption files
merge_annotation_files()
merge_caption_files()

# Define the new base directory for the restructured folders
NEW_BASE_DIR = 'New_Generated_Train'
NEW_IMAGES_DIR = os.path.join(NEW_BASE_DIR, 'Images')
NEW_ANNOTATIONS_DIR = os.path.join(NEW_BASE_DIR, 'Annotations')
NEW_CAPTIONS_DIR = os.path.join(NEW_BASE_DIR, 'Captions')

# Create the new folder structure
os.makedirs(NEW_IMAGES_DIR, exist_ok=True)
os.makedirs(NEW_ANNOTATIONS_DIR, exist_ok=True)
os.makedirs(NEW_CAPTIONS_DIR, exist_ok=True)

# Function to copy files to the new folder structure with renamed folders
def copy_files_to_new_structure(original_dir, new_dir, prefix, file_extension, num_per_prompt, prompts_list):
    file_counter = 1
    for prompt in prompts_list:
        prompt_dir = os.path.join(new_dir, f'{prompt}')
        os.makedirs(prompt_dir, exist_ok=True)
        for j in range(num_per_prompt):
            src_file = os.path.join(original_dir, f'{prefix}-{file_counter}{file_extension}')
            dst_file = os.path.join(prompt_dir, f'{prefix}{j + 1}{file_extension}')
            if os.path.exists(src_file):
                shutil.copy(src_file, dst_file)
            file_counter += 1

# Copy images
for i, prompt in enumerate(prompts):
    prompt_dir = os.path.join(NEW_IMAGES_DIR, f'{prompt}')
    os.makedirs(prompt_dir, exist_ok=True)
    for j in range(NUM_IMAGES_PER_PROMPT):
        src_file = os.path.join(titan_dataset.image_dir, f'{i}_{j}.png')
        dst_file = os.path.join(prompt_dir, f'image{j + 1}.png')
        if os.path.exists(src_file):
            shutil.copy(src_file, dst_file)

# Copy annotations
copy_files_to_new_structure(titan_dataset.annotation_dir, NEW_ANNOTATIONS_DIR, 'object-detect', '.json',
                            NUM_IMAGES_PER_PROMPT, prompts)

# Copy captions
copy_files_to_new_structure(titan_dataset.caption_dir, NEW_CAPTIONS_DIR, 'object-caption', '.json',
                            NUM_IMAGES_PER_PROMPT, prompts)

# Load the Visualizer
#titan_visualizer = TITANViz()

# Interactive Annotation Visualizer
#titan_visualizer.visualize_annotation(image_id=1)

!zip -r /content/test_image.zip /content/New_Generated_Train

pip install Pillow

from PIL import Image
import os

# Path to a single image
image_path = '/content/Data-Generated/images/0_0.png'

# Open the image and get its size
with Image.open(image_path) as img:
    # Get image format
    image_format = img.format
    # Get image size in bytes
    image_size = os.path.getsize(image_path)

# Print image details
print(f'File: {os.path.basename(image_path)}, Format: {image_format}, Size: {image_size / 1024:.2f} KB')

# Calculate the estimated total size for 100,000 images
estimated_total_size = image_size * 100000
print(f'Estimated total size for 100,000 images: {estimated_total_size / (1024 * 1024):.2f} GB')

import os

# Path to a single JSON file
json_path = '/content/New_Generated_Train/Captions/black tea/object-caption1.json'

# Get the size of the JSON file in bytes
json_size_bytes = os.path.getsize(json_path)

# Convert the size to kilobytes (KB)
json_size_kb = json_size_bytes / 1024

print(f'File: {os.path.basename(json_path)}, Size: {json_size_kb:.2f} KB')

# Number of JSON files
num_json_files = 100000  # Example number

# Calculate the estimated total size for all JSON files in KB
estimated_total_size_kb = json_size_kb * num_json_files

# Convert the total size to gigabytes (GB)
estimated_total_size_gb = estimated_total_size_kb / (1024 * 1024)

print(f'Estimated total size for {num_json_files} JSON files: {estimated_total_size_gb:.2f} GB')

!zip New_Generated_Train/

